# trainer configuration for energy and force training
generic:
  min_epochs: 50
  max_epochs: 100
debug:
  accelerator: cpu
  limit_train_batches: 10
  limit_val_batches: 10
  log_every_n_steps: 1
  max_epochs: 2
experiment:
  accelerator: cuda
  strategy:
    class_path: pytorch_lightning.strategies.DDPStrategy
    init_args:
      process_group_backend: "gloo"
      find_unused_parameters: true
callbacks:
  - class_path: pytorch_lightning.callbacks.EarlyStopping
    init_args:
      patience: 10
      monitor: val_force_epoch
      mode: min
      verbose: True
      check_finite: False
  - class_path: matsciml.lightning.callbacks.GradientCheckCallback
  - class_path: matsciml.lightning.callbacks.ExponentialMovingAverageCallback
    init_args:
      decay: 0.99
  - class_path: matsciml.lightning.callbacks.ManualGradientClip
    init_args:
      value: 10.0
      algorithm: "norm"
  - class_path: pytorch_lightning.callbacks.LearningRateMonitor
    init_args:
      logging_interval: step
  - class_path: matsciml.lightning.callbacks.SAM
    init_args:
      rho: 0.05
      adaptive: true
      skip_epoch_count: 30   # start SAM after 30 epochs
  # set the loss scaling so that energy importance ramps up over time
  - class_path: matsciml.lightning.callbacks.LossScalingScheduler
    init_args:
      - class_path: matsciml.lightning.loss_scaling.SigmoidScalingSchedule
        init_args:
          key: "energy"
          initial_value: 1.0
          end_value: 10.0
          center_frac: 0.5
loggers:
  - class_path: pytorch_lightning.loggers.CSVLogger # can omit init_args['save_dir'] for auto directory
  - class_path: pytorch_lightning.loggers.WandbLogger
    init_args:
      log_model: "all"
      project: "matsciml-uip-eval"
      entity: "m3rg"
      mode: "online"
      name: null
      tags:
        - production
        - training
