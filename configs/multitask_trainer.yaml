generic:
  min_epochs: 15
  max_epochs: 1000
debug:
  accelerator: cpu
  limit_train_batches: 10
  limit_val_batches: 10
  log_every_n_steps: 1
  max_epochs: 2
experiment:
  accelerator: cuda
  strategy:
    class_path: pytorch_lightning.strategies.DDPStrategy
    init_args:
      process_group_backend: "gloo"
      find_unused_parameters: true
callbacks:
  - class_path: pytorch_lightning.callbacks.EarlyStopping
    init_args:
      patience: 5
      monitor: val.force
      mode: min
      verbose: True
      check_finite: False
  - class_path: pytorch_lightning.callbacks.ModelCheckpoint
    init_args:
      monitor: val.force
      save_top_k: 3
  - class_path: matsciml.lightning.callbacks.GradientCheckCallback
  - class_path: matsciml.lightning.callbacks.ExponentialMovingAverageCallback
    init_args:
      decay: 0.99
  - class_path: matsciml.lightning.callbacks.ManualGradientClip
    init_args:
      value: 10.0
      algorithm: "norm"
  - class_path: pytorch_lightning.callbacks.LearningRateMonitor
    init_args:
      logging_interval: step
loggers:
  - class_path: pytorch_lightning.loggers.CSVLogger # can omit init_args['save_dir'] for auto directory
  - class_path: pytorch_lightning.loggers.WandbLogger
    init_args:
      log_model: "all"
      project: "matsciml-uip-eval"
      entity: "laserkelvin"
      mode: "online"
      name: null
      tags:
        - production
        - training
        - noisy-nodes
        - multitask
